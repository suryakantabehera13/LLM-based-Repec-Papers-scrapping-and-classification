# -*- coding: utf-8 -*-
"""Repec_Papers_data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aT6n5g08-WVX3op8bGQG-AaSX4eB-Gxh

Extract the working papers data from Repec idea
"""

# Install required packages
!pip install requests beautifulsoup4 pandas scrapy langchain openai tqdm

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
from urllib.parse import urljoin
import logging
from tqdm import tqdm
import random
import torch
import numpy as np

class RepecPaperScraper:
    def __init__(self, base_url="https://ideas.repec.org"):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'AcademicResearchBot/1.0 (+https://example.edu/research)',
            'Accept-Language': 'en-US,en;q=0.5',
        })
        self.papers_data = []
        self.processed_urls = set()
        self.current_page = 1
        self.max_pages = None
        self.series_code = None

    def clean_text(self, text):
        """Clean and normalize text data"""
        if not text:
            return ""
        # Replace common encoding issues
        replacements = {
            'â€“': '-', 'â€˜': "'", 'â€™': "'",
            'â€œ': '"', 'â€': '"', 'Ã©': 'é',
            'â€¢': '-', 'â€¦': '...'
        }
        for k, v in replacements.items():
            text = text.replace(k, v)
        text = re.sub(r'\s+', ' ', text)  # Collapse multiple spaces
        return text.strip()

    def fetch_page(self, url):
        """Fetch a web page with error handling and delays"""
        try:
            time.sleep(random.uniform(1, 3))  # Random delay
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            return response.text if 'text/html' in response.headers.get('Content-Type', '') else None
        except requests.RequestException as e:
            logging.error(f"Error fetching {url}: {e}")
            return None

    def get_next_page_url(self, current_url):
        """Generate next page URL in sequential order"""
        if not current_url:
            return None

        if current_url.endswith('.html'):
            base = current_url.rsplit('.', 1)[0]
            if any(base.endswith(str(i)) for i in range(10)):
                base = base.rstrip('0123456789')
            return f"{base}{self.current_page + 1}.html"
        return None

    def parse_paper_listing(self, html):
        """Parse listing page with complete title extraction"""
        soup = BeautifulSoup(html, 'html.parser')
        papers = []

        for item in soup.select('li.list-group-item.downfree'):
            if not item.text.strip():
                continue

            try:
                # Extract the complete title with ID
                title_tag = item.find('a', href=re.compile(r'^/p/'))
                if not title_tag:
                    continue

                full_text = self.clean_text(title_tag.get_text())

                # Handle both formats: "id:12345 Title" and "Title"
                if full_text.startswith('id:'):
                    paper_id = full_text.split(':')[1].split()[0].strip()
                    paper_title = full_text.split(':')[1].split(' ', 1)[1].strip()
                else:
                    paper_id = ''
                    paper_title = full_text

                # Extract authors
                authors = []
                by_tag = item.find('i', text=re.compile('by', re.I))
                if by_tag and by_tag.next_sibling:
                    authors_text = self.clean_text(by_tag.next_sibling)
                    authors = [a.strip() for a in authors_text.split('&')]

                paper = {
                    'id': paper_id,
                    'title': paper_title,
                    'url': urljoin(self.base_url, title_tag['href']),
                    'authors': authors,
                    'page_number': self.current_page
                }

                if paper['title'] and paper['url']:
                    papers.append(paper)

            except Exception as e:
                logging.error(f"Error parsing item: {e}")

        return papers

    def parse_individual_paper(self, paper):
        """Get detailed info from individual paper page"""
        if not paper.get('url') or paper['url'] in self.processed_urls:
            return None

        html = self.fetch_page(paper['url'])
        if not html:
            return None

        try:
            soup = BeautifulSoup(html, 'html.parser')

            # Get the definitive title from the h1 tag
            title_tag = soup.find('h1')
            if title_tag:
                paper['title'] = self.clean_text(title_tag.get_text())

            # Extract abstract
            abstract = ""
            abstract_div = soup.find('div', id='abstract-body')
            if abstract_div:
                abstract = self.clean_text(abstract_div.get_text())

            # Extract download URL
            download_url = ""
            download_input = soup.find('input', {'type': 'radio', 'checked': True})
            if download_input:
                download_url = download_input.get('value', '')

            # Extract handle
            handle = ""
            handle_tag = soup.find('i', style='word-break:break-all')
            if handle_tag:
                handle = self.clean_text(handle_tag.get_text())

            # Extract keywords
            keywords = []
            more_div = soup.find('div', id='more')
            if more_div:
                keywords = [self.clean_text(kw.get_text()) for kw in
                          more_div.find_all('a', href=re.compile(r'htsearch2'))]

            # Extract suggested citation
            citation = ""
            biblio_div = soup.find('div', id='biblio-body')
            if biblio_div:
                citation_tag = biblio_div.find('li', class_='list-group-item')
                if citation_tag:
                    citation = self.clean_text(citation_tag.get_text())

            # Update paper with all details
            paper.update({
                'abstract': abstract,
                'download_url': download_url,
                'handle': handle,
                'keywords': keywords,
                'suggested_citation': citation
            })

            self.processed_urls.add(paper['url'])
            return paper

        except Exception as e:
            logging.error(f"Error parsing {paper['url']}: {e}")
            return None

    def scrape_series(self, series_code, max_papers=500, max_pages=None):
        """Main scraping function with proper pagination"""
        self.series_code = series_code
        self.max_pages = max_pages
        current_url = f"{self.base_url}/s/{series_code}.html"
        self.current_page = 1
        consecutive_empty = 0

        with tqdm(total=max_papers, desc=f"Scraping {series_code}") as pbar:
            while current_url and len(self.papers_data) < max_papers:
                if self.max_pages and self.current_page > self.max_pages:
                    break

                html = self.fetch_page(current_url)
                if not html:
                    break

                # Check for end of papers
                if "No items" in html or not html.strip():
                    consecutive_empty += 1
                    if consecutive_empty >= 2:
                        break
                    current_url = self.get_next_page_url(current_url)
                    self.current_page += 1
                    continue

                papers = self.parse_paper_listing(html)
                if not papers:
                    consecutive_empty += 1
                    if consecutive_empty >= 2:
                        break
                    current_url = self.get_next_page_url(current_url)
                    self.current_page += 1
                    continue

                consecutive_empty = 0
                for paper in papers:
                    if len(self.papers_data) >= max_papers:
                        break

                    detailed = self.parse_individual_paper(paper)
                    if detailed:
                        self.papers_data.append(detailed)
                        pbar.update(1)

                        # Save checkpoint every 20 papers
                        if len(self.papers_data) % 20 == 0:
                            self._save_checkpoint()

                current_url = self.get_next_page_url(current_url)
                self.current_page += 1

        self._save_checkpoint()
        return [p for p in self.papers_data if p.get('title')]

    def _save_checkpoint(self):
        """Save progress to resume later"""
        try:
            df = pd.DataFrame(self.papers_data)
            # Ensure all expected columns exist
            for col in ['id', 'title', 'authors', 'abstract', 'keywords',
                       'url', 'download_url', 'handle', 'suggested_citation', 'page_number']:
                if col not in df.columns:
                    df[col] = ""

            df.to_csv(f'repec_checkpoint_{self.series_code.replace("/","_")}.csv',
                     index=False, encoding='utf-8-sig')
            logging.info(f"Checkpoint saved with {len(df)} papers")
        except Exception as e:
            logging.error(f"Checkpoint save failed: {e}")

    def save_to_csv(self, filename):
        """Final save with comprehensive data cleaning"""
        clean_data = []
        for paper in self.papers_data:
            # Skip papers without title or URL
            if not paper.get('title') or not paper.get('url'):
                continue

            # Clean all fields
            cleaned = {
                'id': self.clean_text(paper.get('id', '')),
                'title': self.clean_text(paper.get('title', '')),
                'authors': ', '.join([self.clean_text(a) for a in paper.get('authors', [])]),
                'abstract': self.clean_text(paper.get('abstract', '')),
                'keywords': ', '.join([self.clean_text(kw) for kw in paper.get('keywords', [])]),
                'url': paper.get('url', ''),
                'download_url': paper.get('download_url', ''),
                'handle': self.clean_text(paper.get('handle', '')),
                'suggested_citation': self.clean_text(paper.get('suggested_citation', '')),
                'page_number': paper.get('page_number', 0)
            }
            clean_data.append(cleaned)

        if not clean_data:
            logging.warning("No valid papers to save")
            return

        df = pd.DataFrame(clean_data)

        # Ensure consistent column order
        cols = ['id', 'title', 'authors', 'abstract', 'keywords',
                'url', 'download_url', 'handle', 'suggested_citation', 'page_number']
        df = df[cols]

        # Save with UTF-8 encoding to handle special characters
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        logging.info(f"Saved {len(df)} papers to {filename}")

if __name__ == "__main__":
    scraper = RepecPaperScraper()
    series_code = "ess/wpaper"
    #series_code = "iim/iimawp"

    papers = scraper.scrape_series(series_code, max_papers=200)
    #papers = scraper.scrape_series(series_code, max_papers=float('inf'), max_pages=4)
    scraper.save_to_csv("repec_papers_scrap1.csv")

# Load the data from the checkpoint CSV file
try:
    df_checkpoint = pd.read_csv('repec_papers_scrap1.csv')

    with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.expand_frame_repr', False):
        print(df_checkpoint.head())

except FileNotFoundError:
    print("Checkpoint file 'repec_papers_scrap1.csv' not found. Run the scraper first.")
except Exception as e:
    print(f"An error occurred while loading or displaying the data: {e}")

df = df_checkpoint
df.head(600)





"""LLM for filtering economic relevant papers

zero-shot classification model
"""

import os
from transformers import pipeline

# -- Configuration --
ALL_PAPERS_INPUT_FILE = "repec_papers_scrap1.csv"  # CSV input file
FILTERED_OUTPUT_FILE = "repec_papers_economics_filtered.csv"  # CSV output file

def filter_papers_for_economics_colab():
    """
    Reads a CSV file and uses a zero-shot classification model
    to filter for papers related to economics.
    """
    print("\n--- FILTERING PAPERS FOR ECONOMIC RELEVANCE ---")

    if not os.path.exists(ALL_PAPERS_INPUT_FILE):
        print(f"Error: Input CSV file not found at '{ALL_PAPERS_INPUT_FILE}'.")
        return

    print(f"Loading data from {ALL_PAPERS_INPUT_FILE}...")
    df = pd.read_csv(ALL_PAPERS_INPUT_FILE)

    if df.empty:
        print("No papers found in the input file to filter.")
        return

    print(f"Loaded {len(df)} papers to filter.")
    print("Initializing zero-shot classification model...")

    try:
        classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")
        print("Model initialized successfully.")
    except Exception as e:
        print(f"Failed to initialize the model. Please ensure you have an internet connection.")
        print(f"Error: {e}")
        return

    candidate_labels = ["economics", "not economics"]
    CONFIDENCE_THRESHOLD = 0.80
    print(f"Filtering papers with a confidence threshold of {CONFIDENCE_THRESHOLD*100}%...")

    filtered_rows = []

    for i, row in df.iterrows():
        text_to_classify = f"{str(row.get('title', ''))}. {str(row.get('abstract', ''))}"

        if len(text_to_classify.strip()) < 20:
            continue

        try:
            result = classifier(text_to_classify, candidate_labels, multi_label=False)
            top_label = result['labels'][0]
            top_score = result['scores'][0]

            print(f"  ({i+1}/{len(df)}) Classifying '{row.get('title', 'No Title')[:60]}...' -> {top_label} ({top_score:.2f})")

            if top_label == "economics" and top_score >= CONFIDENCE_THRESHOLD:
                filtered_rows.append(row)

        except Exception as e:
            print(f"Could not classify paper: {e}")
            continue

    print(f"\nFiltering complete. Found {len(filtered_rows)} papers relevant to economics.")

    if filtered_rows:
        pd.DataFrame(filtered_rows).to_csv(FILTERED_OUTPUT_FILE, index=False)
        print(f"\n--- Filtered data saved to '{FILTERED_OUTPUT_FILE}' ---")
    else:
        print("No papers met the filtering criteria to be saved.")

# -- Main execution block --
if __name__ == "__main__":
    filter_papers_for_economics_colab()

df3 = pd.read_csv('/content/repec_papers_economics_filtered.csv')
df3



"""sentence embedding + cosine similarity approach"""

!pip install sentence-transformers
from sentence_transformers import SentenceTransformer, util

# Load embedding model (fast, CPU-friendly)
model = SentenceTransformer('all-MiniLM-L6-v2')

# Define economics reference vector
econ_keywords = [
    "economics", "econometric", "microeconomics", "macroeconomics",
    "trade", "GDP", "GNP", "inflation", "deflation", "unemployment",
    "monetary policy", "fiscal policy", "central bank", "interest rate",
    "exchange rate", "trade balance", "productivity", "labor market",
    "supply and demand", "elasticity", "market structure", "game theory",
    "welfare economics", "public economics", "development economics",
    "international trade", "financial economics", "behavioral economics",
    "industrial organization", "economy", "economic growth",
    "recession", "depression", "stimulus", "austerity", "quantitative easing",
    "consumer price index", "purchasing power", "human capital"]
econ_vectors = [model.encode(kw, convert_to_tensor=True) for kw in econ_keywords]
econ_vector = torch.mean(torch.stack(econ_vectors), dim=0)

def is_economics_related(title, abstract, threshold=0.30):
    text = f"{title}. {abstract}"
    text_vector = model.encode(text, convert_to_tensor=True)
    similarity = util.cos_sim(text_vector, econ_vector).item()
    return similarity >= threshold

df = pd.read_csv("repec_papers_scrap1.csv")
df['Is_Economics'] = False

for i in tqdm(df.index):
    title = str(df.loc[i, 'title'])
    abstract = str(df.loc[i, 'abstract'])
    df.loc[i, 'Is_Economics'] = is_economics_related(title, abstract)

df.to_csv("econ_flag_fast.csv", index=False)
df[df['Is_Economics']].to_csv("econ_filtered_fast.csv", index=False)

#df2 = pd.read_csv('/content/econ_filtered_fast.csv')
df2 = pd.read_csv('/content/econ_flag_fast.csv')

df2



"""Annexure: Trying with finetuning with a labeled data"""

from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
from torch.nn.functional import softmax

ECON_KEYWORDS = [
    'economics', 'macroeconomics', 'microeconomics', 'fiscal policy', 'monetary policy',
    'GDP', 'inflation', 'unemployment', 'trade', 'investment', 'finance', 'poverty',
    'development', 'wage', 'budget', 'demand', 'supply', 'market', 'tax', 'inequality'
]

# Load Pretrained Model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForSequenceClassification.from_pretrained(
    "distilbert-base-uncased", num_labels=2
).to(device)
model.eval()

#Helper Functions

def keyword_score(text):
    """Count keyword matches in lowercased text."""
    text = text.lower()
    return sum(kw in text for kw in ECON_KEYWORDS)

def predict_label(text, model, tokenizer, threshold=0.5):
    """Return prediction label and confidence from DistilBERT."""
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)
        probs = softmax(outputs.logits, dim=1)

    econ_confidence = probs[0][1].item()  # 1 = economic, 0 = non-economic
    return econ_confidence, econ_confidence > threshold

def classify_paper(row, keyword_threshold=2, model_threshold=0.5):
    """Classify each paper based on title + abstract + keywords."""
    text = f"{row.get('title', '')}. {row.get('abstract', '')}. {row.get('keywords', '')}"
    kw_score = keyword_score(text)
    model_conf, model_pred = predict_label(text, model, tokenizer, threshold=model_threshold)

    # Hybrid logic: accept if keyword or model passes threshold
    final_label = (kw_score >= keyword_threshold) or model_pred
    return pd.Series({
        "keyword_score": kw_score,
        "model_confidence": round(model_conf, 4),
        "is_economic": final_label
    })

#Load Data
df = pd.read_csv("repec_papers_scrap1.csv")  # Must have title, abstract, keywords

# Classify All Papers
classified = df.apply(classify_paper, axis=1)
df = pd.concat([df, classified], axis=1)

# Save Results
df.to_csv("all_papers_with_predictions.csv", index=False)
df[df["is_economic"]].to_csv("economic_papers_only.csv", index=False)

print(f"""
 Classification Complete:
- Total papers processed: {len(df)}
- Economic papers found: {df['is_economic'].sum()}
- Saved to: all_papers_with_predictions.csv
- Filtered to: economic_papers_only.csv
""")

from transformers import (
    DistilBertTokenizer, DistilBertForSequenceClassification,
    Trainer, TrainingArguments
)
from datasets import Dataset
from sklearn.model_selection import train_test_split
from torch.nn.functional import softmax

# Keyword bank
ECON_KEYWORDS = [
    'economics', 'macroeconomics', 'microeconomics', 'fiscal policy', 'monetary policy',
    'GDP', 'inflation', 'unemployment', 'trade', 'investment', 'finance', 'poverty',
    'development', 'wage', 'budget', 'demand', 'supply', 'market', 'tax', 'inequality'
]

# Load tokenizer & model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForSequenceClassification.from_pretrained(
    "distilbert-base-uncased", num_labels=2
).to(device)

# Load labeled data
df = pd.read_csv("labeled_data.csv")  # expects "text" and "label" columns (label: 0 or 1)

# Split for training and eval
train_df, eval_df = train_test_split(df, test_size=0.2, random_state=42)

# Tokenize
def tokenize(batch):
    return tokenizer(batch['text'], padding="max_length", truncation=True, max_length=512)

train_dataset = Dataset.from_pandas(train_df).map(tokenize, batched=True).rename_column("label", "labels")
eval_dataset = Dataset.from_pandas(eval_df).map(tokenize, batched=True).rename_column("label", "labels")

train_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
eval_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])

# Fine-tuning config
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    logging_steps=5
)


trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset
)
import os
os.environ["WANDB_MODE"] = "offline"

# Train
trainer.train()
trainer.save_model("fine_tuned_model.pt")

print("\n Fine-tuning complete. Saved as 'fine_tuned_model.pt'.")





API_KEY = "AIzaSyCFqzuKWl99KX-ewf02-zjJtZrGhOoMnnk"
MODEL = "gemini-1.5-flash"  # Based on your working curl example
ENDPOINT = f"https://generativelanguage.googleapis.com/v1beta/models/{MODEL}:generateContent?key={API_KEY}"
HEADERS = {
    "Content-Type": "application/json"
}

df = pd.read_csv("repec_papers_scrap1.csv")
print(f"Total papers: {len(df)}")

def build_prompt(title, abstract):
    return f"""
You are an expert classifier.

Given the following academic paper, decide whether it's related to economics:

Title: "{title}"
Abstract: "{abstract}"

Answer with one word only:
- ECONOMICS
- NOT ECONOMICS
- MAYBE
"""

def call_gemini(prompt):
    payload = {
        "contents": [
            {
                "parts": [{"text": prompt}]
            }
        ]
    }

    try:
        response = requests.post(ENDPOINT, headers=HEADERS, json=payload)
        response.raise_for_status()
        content = response.json()
        reply = content['candidates'][0]['content']['parts'][0]['text']
        return reply.strip().upper()
    except Exception as e:
        print(f"Error: {e}")
        return "ERROR"

df1 = df.sample(15).reset_index(drop=True)

results = []

for i, row in tqdm(df1.iterrows(), total=len(df1)):
    title = str(row.get("title", "")).strip()
    abstract = str(row.get("abstract", "")).strip()

    if not title and not abstract:
        results.append("SKIP")
        continue

    prompt = build_prompt(title, abstract)
    label = call_gemini(prompt)
    results.append(label)

    #time.sleep(0.5)  # Prevent throttling
    time.sleep(random.uniform(3, 6))  # instead of 0.5 seconds

df1["Gemini_Label"] = results
df1.to_csv("gemini_classified_papers.csv", index=False)
df1["Gemini_Label"].value_counts()

econ_df = df1[df1["Gemini_Label"] == "ECONOMICS"]
econ_df.to_csv("gemini_economics_only.csv", index=False)
print(f"Economic papers found: {len(econ_df)}")



!pip install together

from together import Together

# Setup
client = Together(api_key="eaf747aedd45cd3cf595a60b828e83e74da052f90cf339164cdd97aa4b8bf412")
model_name = "meta-llama/Llama-Vision-Free"
#model_name = "meta-llama/Llama-3.3-70B-Instruct-Turbo-Free"

df = pd.read_csv("repec_papers_scrap1.csv")
df = df.sample(20).reset_index(drop=True)  # Test on 20 samples

labels = []

# Loop through rows
for i, row in tqdm(df.iterrows(), total=len(df)):
    title = str(row.get("title", ""))
    abstract = str(row.get("abstract", ""))

    prompt = f"""
You are a research assistant for an academic economics journal. Your job is to classify whether a research paper is relevant to the field of economics or not. Use only the title and abstract.

Here are some examples:

---
Title: Deep Learning for Protein Folding
Abstract: This paper explores transformer models for predicting 3D protein structures.
Answer: not economics
---
Title: Inflation Dynamics in Latin America
Abstract: We study inflation volatility across developing countries using central bank interventions.
Answer: economics
---
Title: Blockchain Scalability Techniques
Abstract: We propose a sharding mechanism to improve blockchain throughput.
Answer: not economics
---
Title: {title}
Abstract: {abstract}
Answer:"""

    try:
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
            max_tokens=20
        )

        output = response.choices[0].message.content.strip().lower()

        if "not economics" in output:
            labels.append("not economics")
        elif "economics" in output:
            labels.append("economics")
        else:
            print(f"[Warning] Unexpected response for row {i}: {output}")
            labels.append("unknown")

    except Exception as e:
        print(f"[Error] Failed at row {i}: {e}")
        labels.append("error")

    time.sleep(random.uniform(2.5, 4.5))  # Add slight delay to avoid rate limits

# Attach labels and save
df["Llama3_Label"] = labels
df.to_csv("llama3_classified_papers.csv", index=False)
print(df["Llama3_Label"].value_counts())

df2 = pd.read_csv("llama3_classified_papers.csv")
df2



!pip install cohere --upgrade

import cohere
import json
from datetime import datetime

# Initialize Cohere client
co = cohere.Client("TwhGiR7WORj1YvE1wppqAqMH5ahMuFEP20SVpYge")

def classify_with_cohere(title, abstract):
    """Classify paper using Cohere's API with structured output"""
    prompt = f"""
    Analyze this academic paper and determine if it's primarily about economics.
    Respond in JSON format with these keys:
    - is_economic (boolean)
    - confidence_score (float 0-1)
    - reasoning (string)
    - economic_subfields (list of relevant subfields if economic)

    Evaluation Criteria:
    1. Studies economic systems, markets, or behavior
    2. Uses economic theories/methods
    3. Published in economics context

    Paper Title: {title}
    Abstract: {abstract}

    Example Response:
    {{
        "is_economic": true,
        "confidence_score": 0.94,
        "reasoning": "The paper analyzes labor market dynamics using econometric methods",
        "economic_subfields": ["Labor Economics", "Econometrics"]
    }}
    """

    try:
        response = co.chat(
            model="command",
            message=prompt,
            temperature=0.3,
            preamble="You are an economics professor evaluating research papers."
        )

        # Extract JSON from response text
        json_str = response.text
        json_str = json_str[json_str.find('{'):json_str.rfind('}')+1]
        return json.loads(json_str)

    except Exception as e:
        print(f"Classification error: {str(e)}")
        return {
            "is_economic": None,
            "confidence_score": None,
            "reasoning": f"Error: {str(e)}",
            "economic_subfields": []
        }

# Load papers data
df = pd.read_csv('repec_papers_scrap1.csv')
df = df.sample(20).reset_index(drop=True)  # Test on 20 samples

# Prepare new columns
df['is_economic'] = None
df['economic_confidence'] = None
df['classification_reason'] = None
df['economic_subfields'] = None
df['classification_time'] = None

# Track request timestamps
request_timestamps = []

# Classify papers with rate limiting
for idx, row in tqdm(df.iterrows(), total=len(df)):
    if pd.isna(row['title']) or pd.isna(row['abstract']):
        continue

    # Implement rate limiting (10 requests per minute)
    while len(request_timestamps) >= 10:
        oldest_request = request_timestamps[0]
        if (datetime.now() - oldest_request).total_seconds() < 60:
            sleep_time = 60 - (datetime.now() - oldest_request).total_seconds()
            print(f"Rate limit reached. Sleeping for {sleep_time:.1f} seconds...")
            time.sleep(sleep_time)
        request_timestamps.pop(0)

    # Record request time
    request_timestamps.append(datetime.now())

    # Classify paper
    result = classify_with_cohere(row['title'], row['abstract'])

    # Store results
    df.at[idx, 'is_economic'] = result.get('is_economic')
    df.at[idx, 'economic_confidence'] = result.get('confidence_score')
    df.at[idx, 'classification_reason'] = result.get('reasoning')
    df.at[idx, 'economic_subfields'] = ', '.join(result.get('economic_subfields', []))
    df.at[idx, 'classification_time'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # Save progress every 5 papers
    if idx % 5 == 0:
        df.to_csv('repec_cohere_classified.csv', index=False)
        print(f"Saved progress at row {idx}")

# Final save
df.to_csv('repec_cohere_classified.csv', index=False)
print("Classification complete!")

df2 = pd.read_csv("repec_cohere_classified.csv")
df2





# Setup
client = Together(api_key="eaf747aedd45cd3cf595a60b828e83e74da052f90cf339164cdd97aa4b8bf412")
#model_name = "meta-llama/Llama-Vision-Free"
model_name = "meta-llama/Llama-3-8b-chat-hf" # Using a smaller, generally available model

df = pd.read_csv("repec_papers_scrap1.csv")
df = df.sample(20).reset_index(drop=True)  # Test on 20 samples

labels = []

# Loop through rows
for i, row in tqdm(df.iterrows(), total=len(df)):
    title = str(row.get("title", ""))
    abstract = str(row.get("abstract", ""))

    prompt = f"""
You are an expert academic classifier with a deep understanding of economic research. Your task is to determine if the following research paper is relevant to the field of economics.

- **Relevant to economics** means the paper's primary focus involves economic principles, theories, or analysis. This includes, but is not limited to:
    - **Core Theory:** Microeconomics, Macroeconomics, Econometrics, Game Theory.
    - **Applied Fields:** Development Economics, Labor Economics, Public Finance, International Trade, Industrial Organization, Financial Economics.
    - **Interdisciplinary Areas:** Behavioral Economics, Health Economics, Environmental Economics, Law and Economics, Economic History, Political Economy.
    - **Methodology:** The paper uses quantitative models, statistical analysis (like regression), or economic modeling to analyze human behavior, resource allocation, or market outcomes.

- **Not relevant** means the paper's primary focus is on another discipline (e.g., pure sociology, agronomy, clinical studies) without a significant economic analysis component.

**Analyze the provided title and abstract to make your classification.**

**Output only "Relevant" or "Not Relevant".**

**Paper Data:**
{{
  "title": "{title}",
  "abstract": "{abstract}"
}}

Title: {title}
Abstract: {abstract}
Answer:"""

    try:
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
            max_tokens=20
        )

        output = response.choices[0].message.content.strip().lower()

        if "not relevant" in output:
            labels.append("not economics")
        elif "relevant" in output:
            labels.append("economics")
        else:
            print(f"[Warning] Unexpected response for row {i}: {output}")
            labels.append("unknown")

    except Exception as e:
        print(f"[Error] Failed at row {i}: {e}")
        labels.append("error")

    time.sleep(random.uniform(2.5, 4.5))  # Add slight delay to avoid rate limits

# Attach labels and save
df["Llama3_Label"] = labels
df.to_csv("llama3_classified_papers.csv", index=False)
print(df["Llama3_Label"].value_counts())

df2 = pd.read_csv("llama3_classified_papers.csv")
df2

